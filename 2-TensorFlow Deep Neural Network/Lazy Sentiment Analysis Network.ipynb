{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Sentiment Analysis Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same model as before lets see how it performs on a totally different type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('./data/imdb/reviews.txt', header=None)\n",
    "labels = pd.read_csv('./data/imdb/labels_ohe.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of unique words in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = Counter()\n",
    "for i,row in reviews.iterrows():\n",
    "    total_counts.update(row[0].split(' '))\n",
    "\n",
    "print(\"Total number of unique words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place all the unique words in the dataset in a list, sorted by most frequent word first. This is the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000]\n",
    "print(vocab[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the number of occurences of one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[77], ': ', total_counts[vocab[77]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign an index to each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)} #dictionary comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts text to a vector that indicates the occurance of a word in the vocabulary. However, it does not count the number of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(len(vocab), dtype=np.int_)\n",
    "    for word in text.split(' '):\n",
    "        idx = word2idx.get(word,None)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[idx] = 1 # was += 1\n",
    "    return np.array(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector('There were lots of good movies and stars this year')[:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dataset 25000 text reviews to 25000 word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the first 25 elements of the first 5 word vectors\n",
    "print(word_vectors[:5, :25])\n",
    "print(reviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_records = len(labels)\n",
    "shuffle = np.arange(number_of_records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "#making a train / test split\n",
    "train_split, test_split = shuffle[:int(number_of_records*test_fraction)], shuffle[int(number_of_records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], labels.values[train_split,:]\n",
    "testX, testY = word_vectors[test_split,:], labels.values[test_split]\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n",
    "type(testX[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a quick batch system\n",
    "def get_next_batch(batch_size,i):\n",
    "    return trainX[(i*batch_size):((i+1)*batch_size)].astype('float32'),trainY[(i*batch_size):((i+1)*batch_size)].astype('float32')\n",
    "\n",
    "# Testing\n",
    "batch_x, batch_y = get_next_batch(100,3)\n",
    "print(batch_x[0:5])\n",
    "print(batch_y[0:5])\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper Parameters \n",
    "learning_rate = 0.001 \n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1  # for how often to print out our results\n",
    "model_path = \"./models_sentiment/model_sentiment.ckpt\"\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 10000 # we now have 10k vectors of our words\n",
    "n_hidden_1 = 384 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_classes = 2 # 2 classes for predicting positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   # don't allow session to take up all the GPU memory\n",
    "\n",
    "# The graph\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input],name='X_Input')\n",
    "y = tf.placeholder(\"float\", [None, n_classes],name='Y_Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \n",
    "    # Hidden layer 01 with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])  # adding (x + w1 + bias1)\n",
    "    layer_1 = tf.nn.relu(layer_1, name='Layer1_Relu') #activation\n",
    "    \n",
    "    # Hidden layer 02 with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2, name='Layer2_Relu')\n",
    "    \n",
    "    # Logits layer with linear activation\n",
    "    logits_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    #logits_layer = tf.nn.softmax(logits_layer)\n",
    "    return logits_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev = 0.1)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev = 0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],stddev = 0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.ones([n_hidden_1])/10),\n",
    "    'b2': tf.Variable(tf.ones([n_hidden_2])/10),\n",
    "    'out': tf.Variable(tf.ones([n_classes])/10)\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "# this is were we compute error against the correct results\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))*100\n",
    "\n",
    "# optimizer made to change weights and biases to optimize cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_y = tf.summary.scalar('output', y)\n",
    "file_writer = tf.summary.FileWriter('log_simple_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(trainX.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = get_next_batch(100,i)\n",
    "            #print(batch_x[0])\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_x, y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"Loss =\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Training Finished!\")\n",
    "    \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print (\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Restore model weights from previously saved model\n",
    "    load_path = saver.restore(sess, model_path)\n",
    "    print (\"Model restored from file: %s\" % save_path)\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: testX, y: testY}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text_string):\n",
    "    textVec= text_to_vector(text_string)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Restore model weights from previously saved model\n",
    "        load_path = saver.restore(sess, model_path)\n",
    "        # Predict model 1 image batch size = 1\n",
    "        vector = sess.run(pred, feed_dict={x: [textVec]})\n",
    "        #print(vector[0])\n",
    "        pred_label = sess.run(tf.argmax(vector[0],0))\n",
    "        #print the label\n",
    "        if pred_label == 1: \n",
    "            print('Positive')\n",
    "        else:\n",
    "            print('Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_text(\"lion is a great movie to watch this year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_text(\"this was worst experience in a long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
